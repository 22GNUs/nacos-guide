# 第4节.Nacos服务端服务注册之Distro协议集群注册同步与异步复制

Distro协议会在集群之间进行实例数据的同步。V1版本主要采用HTTP的方式。V2版本主要采用GRPC的方式。本篇文章主要介绍基于AP模式下的Nacos的集群之间的数据同步处理。

## 一、V1版本的HTTP方式

### 1.1、当前负责节点发送Distro请求源码

#### 1.1.1、Distro注册信息定义

从上一节的梳理中我们知道，在HTTP注册实例的时候，会调用consistencyService.put(key, instances);方法进行一致性协议的处理。

对于V2版本来说，在这里会判断当前实例是否为临时实例，如果是临时实例，同时如果是支持GRPC请求，则不执行下方的协议同步处理，执行如下代码：

```java
public void put(String key, Record value) throws NacosException {
    onPut(key, value);
    // If upgrade to 2.0.X, do not sync for v1.
    if (ApplicationUtils.getBean(UpgradeJudgement.class).isUseGrpcFeatures()) {
        return;
    }
    distroProtocol.sync(new DistroKey(key, KeyBuilder.INSTANCE_LIST_KEY_PREFIX), DataOperation.CHANGE,
            DistroConfig.getInstance().getSyncDelayMillis());
}
```

如果是持久化实例，则执行PersistentServiceProcessor类中的PUT方法，进行基于RAFT的模式同步。

对于这块的源码阅读，可以参考Nacos1.4.3的源码实现会比较清晰，V2版本出现了多个子类，可能会影响你的阅读。这里我们忽略是否支持GRPC的判断处理，学习一下V1版本的distroProtocol.sync的业务处理。

通过如下代码，可以看到distroProtocol类中的sync方法，排除了当前服务器，向其他服务器发送Distro请求处理。

```java
public void sync(DistroKey distroKey, DataOperation action, long delay) {
    for (Member each : memberManager.allMembersWithoutSelf()) {
        syncToTarget(distroKey, action, each.getAddress(), delay);
    }
}
```

#### 1.1.2、异步复制Task任务与数据复制引擎

syncToTarget方法内部又调用了NacosDelayTaskExecuteEngine这个核心的任务执行引擎去处理，最终会执行到DistroDelayTaskProcessor类中，进而会调用DistroSyncChangeTask类run方法：

```java
@Override
public void run() {
    Loggers.DISTRO.info("[DISTRO-START] {}", toString());
    try {
        String type = getDistroKey().getResourceType();
        DistroData distroData = distroComponentHolder.findDataStorage(type).getDistroData(getDistroKey());
        distroData.setType(DataOperation.CHANGE);
        boolean result = distroComponentHolder.findTransportAgent(type).syncData(distroData, getDistroKey().getTargetServer());
        if (!result) {
            handleFailedTask();
        }
        Loggers.DISTRO.info("[DISTRO-END] {} result: {}", toString(), result);
    } catch (Exception e) {
        Loggers.DISTRO.warn("[DISTRO] Sync data change failed.", e);
        handleFailedTask();
    }
}
```

在HTTP的执行逻辑中，会执行到DistroHttpAgent的syncData方法，从而向其他服务节点发送同步请求。

```java
@Override
public boolean syncData(DistroData data, String targetServer) {
    if (!memberManager.hasMember(targetServer)) {
        return true;
    }
    byte[] dataContent = data.getContent();
    return NamingProxy.syncData(dataContent, data.getDistroKey().getTargetServer());
}
```

对于真正执行请求的方法，代码如下：

```java
public static boolean syncData(byte[] data, String curServer) {
    Map<String, String> headers = new HashMap<>(128);
    
    headers.put(HttpHeaderConsts.CLIENT_VERSION_HEADER, VersionUtils.version);
    headers.put(HttpHeaderConsts.USER_AGENT_HEADER, UtilsAndCommons.SERVER_VERSION);
    headers.put(HttpHeaderConsts.ACCEPT_ENCODING, "gzip,deflate,sdch");
    headers.put(HttpHeaderConsts.CONNECTION, "Keep-Alive");
    headers.put(HttpHeaderConsts.CONTENT_ENCODING, "gzip");
    
    try {
        RestResult<String> result = HttpClient.httpPutLarge(
                "http://" + curServer + EnvUtil.getContextPath() + UtilsAndCommons.NACOS_NAMING_CONTEXT
                        + DATA_ON_SYNC_URL, headers, data);
        if (result.ok()) {
            return true;
        }
        if (HttpURLConnection.HTTP_NOT_MODIFIED == result.getCode()) {
            return true;
        }
        throw new IOException("failed to req API:" + "http://" + curServer + EnvUtil.getContextPath()
                + UtilsAndCommons.NACOS_NAMING_CONTEXT + DATA_ON_SYNC_URL + ". code:" + result.getCode() + " msg: "
                + result.getData());
    } catch (Exception e) {
        Loggers.SRV_LOG.warn("NamingProxy", e);
    }
    return false;
}
```

可以看到他向其他节点发送了HTTP的distro协议复制请求。地址为：/nacos/v1/ns/distro/datum

### 1.2、其他非负责节点接收Distro复制请求源码

对于HTTP类型的Distro请求，服务端会先进入到DistroFilter类中进行过滤处理。

然后DistroController类会进行接收处理，接口地址为/nacos/v1/ns/distro/datum，源码如下：

```java
@PutMapping("/datum")
public ResponseEntity onSyncDatum(@RequestBody Map<String, Datum<Instances>> dataMap) throws Exception {
    
    if (dataMap.isEmpty()) {
        Loggers.DISTRO.error("[onSync] receive empty entity!");
        throw new NacosException(NacosException.INVALID_PARAM, "receive empty entity!");
    }
    
    for (Map.Entry<String, Datum<Instances>> entry : dataMap.entrySet()) {
        if (KeyBuilder.matchEphemeralInstanceListKey(entry.getKey())) {
            String namespaceId = KeyBuilder.getNamespace(entry.getKey());
            String serviceName = KeyBuilder.getServiceName(entry.getKey());
            if (!serviceManager.containService(namespaceId, serviceName) && switchDomain
                    .isDefaultInstanceEphemeral()) {
                serviceManager.createEmptyService(namespaceId, serviceName, true);
            }
            DistroHttpData distroHttpData = new DistroHttpData(createDistroKey(entry.getKey()), entry.getValue());
            distroProtocol.onReceive(distroHttpData);
        }
    }
    return ResponseEntity.ok("ok");
}
```

在协议层的接收处理就很简单了，源码如下：

```java
public boolean processData(DistroData distroData) {
    DistroHttpData distroHttpData = (DistroHttpData) distroData;
    Datum<Instances> datum = (Datum<Instances>) distroHttpData.getDeserializedContent();
    onPut(datum.key, datum.value);
    return true;
}
```

从源码中可以看到，这个实例的元数据信息存储到了内存的Datum中。相对于负责节点的实例注册，没有了sync的过程。

## 二、V2版本的GRPC方式

在Nacos2.x版本中，采用GRPC的方式进行集群实例的Distro协议的同步与复制，并设计了Client模型来完成同步。所以在Nacos2.x中，client可能是真实的实例客户端，也可能是集群中的某一服务节点。

###  2.1、当前负责节点发送Distro请求源码

在之前服务端的注册逻辑中，代码中调用了client.addServiceInstance(singleton, instanceInfo)代码，当前是AP模型的情况下，在Nacos中是将服务信息放入到了client和ServiceManager中。核心是派发了ClientChangedEvent事件：

```java
  NotifyCenter.publishEvent(new ClientEvent.ClientChangedEvent(this));
```

然后会异步的将注册的服务信息同步和集群其他节点，而这个事件的处理在`DistroClientDataProcessor`，当前节点作为客户端给其他节点服务器发送Distro数据，核心代码如下：

```java
@Override
public void onEvent(Event event) {
    if (EnvUtil.getStandaloneMode()) {
        return;
    }
    if (!upgradeJudgement.isUseGrpcFeatures()) {
        return;
    }
    if (event instanceof ClientEvent.ClientVerifyFailedEvent) {
        syncToVerifyFailedServer((ClientEvent.ClientVerifyFailedEvent) event);
    } else {
        syncToAllServer((ClientEvent) event);
    }
}

private void syncToAllServer(ClientEvent event) {
        Client client = event.getClient();
        // Only ephemeral data sync by Distro, persist client should sync by raft.
        if (null == client || !client.isEphemeral() || !clientManager.isResponsibleClient(client)) {
            return;
        }
        if (event instanceof ClientEvent.ClientDisconnectEvent) {
            DistroKey distroKey = new DistroKey(client.getClientId(), TYPE);
            distroProtocol.sync(distroKey, DataOperation.DELETE);
        } else if (event instanceof ClientEvent.ClientChangedEvent) {
            DistroKey distroKey = new DistroKey(client.getClientId(), TYPE);
          	//核心代码逻辑 调用统一的distro的sync方法
            distroProtocol.sync(distroKey, DataOperation.CHANGE);
        }
    }
```

#### 2.1.1、异步复制Task任务与数据复制引擎

在V2版本的代码中， distroProtocol.sync的数据同步任务，会触发DelayTaskExecuteEngine任务异步复制引擎，去同步数据，代码：

```java
public void syncToTarget(DistroKey distroKey, DataOperation action, String targetServer, long delay) {
    DistroKey distroKeyWithTarget = new DistroKey(distroKey.getResourceKey(), distroKey.getResourceType(),
            targetServer);
    DistroDelayTask distroDelayTask = new DistroDelayTask(distroKeyWithTarget, action, delay);
    distroTaskEngineHolder.getDelayTaskExecuteEngine().addTask(distroKeyWithTarget, distroDelayTask);
    if (Loggers.DISTRO.isDebugEnabled()) {
        Loggers.DISTRO.debug("[DISTRO-SCHEDULE] {} to {}", distroKey, targetServer);
    }
}
```

进而会执行到DistroSyncChangeTask父类AbstractDistroExecuteTask中的run方法，选择当前支持的Distro的传输代理，代码如下：

```java
@Override
public void run() {
    String type = getDistroKey().getResourceType();
  	//根据当前类型选择是HTTP还是GRPC的distro传输协议代理
    DistroTransportAgent transportAgent = distroComponentHolder.findTransportAgent(type);
    if (null == transportAgent) {
        Loggers.DISTRO.warn("No found transport agent for type [{}]", type);
        return;
    }
    Loggers.DISTRO.info("[DISTRO-START] {}", toString());
    if (transportAgent.supportCallbackTransport()) {
        doExecuteWithCallback(new DistroExecuteCallback());
    } else {
        executeDistroTask();
    }
}
```

从而回调DistroSyncChangeTask类的doExecute方法，然后就会调用DistroClientTransportAgent类进行GRPC方式的数据同步，代码如下：

```java
@Override
public boolean syncData(DistroData data, String targetServer) {
  	//检查目标服务器是否存储
    if (isNoExistTarget(targetServer)) {
        return true;
    }
    DistroDataRequest request = new DistroDataRequest(data, data.getType());
  	//从集寻址配置中获得寻址信息
    Member member = memberManager.find(targetServer);
  	//检查当前服务的健康程度
    if (checkTargetServerStatusUnhealthy(member)) {
        Loggers.DISTRO.warn("[DISTRO] Cancel distro sync caused by target server {} unhealthy", targetServer);
        return false;
    }
    try {
      	//发送GRPC请求进行同步数据
        Response response = clusterRpcClientProxy.sendRequest(member, request);
        return checkResponse(response);
    } catch (NacosException e) {
        Loggers.DISTRO.error("[DISTRO-FAILED] Sync distro data failed! ", e);
    }
    return false;
}
```

#### 2.1.2、客户端订阅通知

在V2版本的代码中，在服务端调用服务注册代码后，还额外会触发通知ServiceChangedEvent事件，这个事件的订阅者为：
NamingSubscriberServiceV2Impl，该类负责处理ServiceChangedEvent和ServiceSubscribedEvent，代码如下：

```java
@Override
public List<Class<? extends Event>> subscribeTypes() {
    List<Class<? extends Event>> result = new LinkedList<>();
    result.add(ServiceEvent.ServiceChangedEvent.class);
    result.add(ServiceEvent.ServiceSubscribedEvent.class);
    return result;
}
```

该事件主要会负责如下工作：

1.通知订阅客户端

2.Nacos集群数据同步。

这样当Nacos服务端的某一台服务出现变更以后，会通过GRPC的方式向服务端发送请求，然后会发布`ServiceEvent.ServiceChangedEvent`事件，这个事件最后会在`NamingSubscriberServiceV2Impl.onEvent`中处理，会通过push方式（gRPC)将新上线的服务信息推送给消费方。

### 2.2、其他非负责节点接收Distro负责请求源码

#### 2.2.1、GRPC服务端启动流程

在Nacos服务器启动的时候，会开启GRPC的服务端端口，默认是在当前端口上偏移1000端口号，核心逻辑在BaseRpcServer类中，该类中的start方法上使用了Spring的@PostConstruct方法，进行项目启动的入口点，核心代码如下：

```java
@PostConstruct
public void start() throws Exception {
    String serverName = getClass().getSimpleName();
    Loggers.REMOTE.info("Nacos {} Rpc server starting at port {}", serverName, getServicePort());
    
    startServer();

    Loggers.REMOTE.info("Nacos {} Rpc server started at port {}", serverName, getServicePort());
    Runtime.getRuntime().addShutdownHook(new Thread(() -> {
        Loggers.REMOTE.info("Nacos {} Rpc server stopping", serverName);
        try {
            BaseRpcServer.this.stopServer();
            Loggers.REMOTE.info("Nacos {} Rpc server stopped successfully...", serverName);
        } catch (Exception e) {
            Loggers.REMOTE.error("Nacos {} Rpc server stopped fail...", serverName, e);
        }
    }));

}
```

#### 2.2.2、GRPC服务端接收请求处理

当客户端已经连接负责节点的服务器后，然后发送了Distro的协议进行数据同步，该节点会向集群中的其他服务器发送Distro数据，作为集群中的Nacos节点的接收方的GRPC处理，会被GRPC的接收处理器所处理：

```java
@Autowired
private GrpcRequestAcceptor grpcCommonRequestAcceptor;
```

在GrpcRequestAcceptor类中负责接收各种GRPC的请求，从而根据当前请求拿到具体的请求实现处理类：

```java
RequestHandler requestHandler = requestHandlerRegistry.getByRequestType(type);
```

从而进入到DistroDataRequestHandler类中处理GRPC的DistroData数据同步请求，进而会执行到核心的接收数据处理中：

```java
distroProtocol.onReceive(distroData)
```

在接下来的DistroClientDataProcessor类中，对于V2版本的GRPC的核心接收同步代码如下，进行当前服务节点的client模型的数据更新：

```java
private void handlerClientSyncData(ClientSyncData clientSyncData) {
    Loggers.DISTRO.info("[Client-Add] Received distro client sync data {}", clientSyncData.getClientId());
    clientManager.syncClientConnected(clientSyncData.getClientId(), clientSyncData.getAttributes());
    Client client = clientManager.getClient(clientSyncData.getClientId());
    upgradeClient(client, clientSyncData);
}

private void upgradeClient(Client client, ClientSyncData clientSyncData) {
    List<String> namespaces = clientSyncData.getNamespaces();
    List<String> groupNames = clientSyncData.getGroupNames();
    List<String> serviceNames = clientSyncData.getServiceNames();
    List<InstancePublishInfo> instances = clientSyncData.getInstancePublishInfos();
    Set<Service> syncedService = new HashSet<>();
    for (int i = 0; i < namespaces.size(); i++) {
        Service service = Service.newService(namespaces.get(i), groupNames.get(i), serviceNames.get(i));
        Service singleton = ServiceManager.getInstance().getSingleton(service);
        syncedService.add(singleton);
        InstancePublishInfo instancePublishInfo = instances.get(i);
        if (!instancePublishInfo.equals(client.getInstancePublishInfo(singleton))) {
            client.addServiceInstance(singleton, instancePublishInfo);
            NotifyCenter.publishEvent(
                    new ClientOperationEvent.ClientRegisterServiceEvent(singleton, client.getClientId()));
        }
    }
    for (Service each : client.getAllPublishedService()) {
        if (!syncedService.contains(each)) {
            client.removeServiceInstance(each);
            NotifyCenter.publishEvent(
                    new ClientOperationEvent.ClientDeregisterServiceEvent(each, client.getClientId()));
        }
    }
}
```



